{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f9a43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTForImageClassification\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.stats import norm, laplace, t, lognorm, cauchy, pareto\n",
    "\n",
    "# --- Configuration --- #\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SAVE_DIR = \"gradient_histograms_combined\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Data Loader --- #\n",
    "def get_data_loader():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# --- Model and Optimizer --- #\n",
    "def setup_model():\n",
    "    model = ViTForImageClassification.from_pretrained(MODEL_NAME)\n",
    "    model.classifier = nn.Linear(model.classifier.in_features, NUM_CLASSES)\n",
    "    return model.to(DEVICE), optim.AdamW(model.parameters(), lr=LR), nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Gradient Hooking --- #\n",
    "def register_hooks(model, storage):\n",
    "    def save_grad(name):\n",
    "        def hook(_, __, grad_output):\n",
    "            if grad_output[0] is not None:\n",
    "                storage[name].append(grad_output[0].detach().cpu().flatten())\n",
    "        return hook\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d, nn.LayerNorm)):\n",
    "            module.register_full_backward_hook(save_grad(name))\n",
    "\n",
    "# --- Training --- #\n",
    "def train(model, loader, optimizer, criterion, gradient_storage):\n",
    "    model.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, Loss: {loss.item():.4f}\")\n",
    "            if idx == 10:\n",
    "                return\n",
    "\n",
    "# --- Q-Q Plot Utility --- #\n",
    "def compute_qq(data, dist, absval=False):\n",
    "    data = data.cpu().numpy()\n",
    "    if absval:\n",
    "        data = np.abs(data)\n",
    "    data = data[np.isfinite(data) & (data != 0)]\n",
    "    probs = np.linspace(0.01, 0.99, 100)\n",
    "    empirical = np.quantile(data, probs)\n",
    "\n",
    "    if dist == 'normal':\n",
    "        theoretical = norm.ppf(probs)\n",
    "    elif dist == 'laplace':\n",
    "        theoretical = laplace.ppf(probs)\n",
    "    elif dist == 't':\n",
    "        theoretical = t.ppf(probs, df=3)\n",
    "    elif dist == 'lognorm':\n",
    "        shape, loc, scale = lognorm.fit(data, floc=0)\n",
    "        theoretical = lognorm.ppf(probs, shape, loc=loc, scale=scale)\n",
    "    elif dist == 'cauchy':\n",
    "        loc, scale = cauchy.fit(data)\n",
    "        theoretical = cauchy.ppf(probs, loc=loc, scale=scale)\n",
    "    elif dist == 'pareto':\n",
    "        b, loc, scale = pareto.fit(data, floc=0)\n",
    "        theoretical = pareto.ppf(probs, b, loc=loc, scale=scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {dist}\")\n",
    "\n",
    "    return theoretical, empirical\n",
    "\n",
    "# --- Plotting --- #\n",
    "def plot_gradients(storage, bins=50):\n",
    "    grouped = defaultdict(dict)\n",
    "    for name in storage:\n",
    "        match = re.search(r'encoder\\.layer\\.(\\d+)\\.(.*?)\\.(.*)', name)\n",
    "        if match:\n",
    "            lid, sub, param = match.groups()\n",
    "            grouped[f\"layer_{lid}_{sub}\"][param] = name\n",
    "        else:\n",
    "            grouped[\"classifier_head\"][name] = name\n",
    "\n",
    "    for gname, parts in grouped.items():\n",
    "        fig, axs = plt.subplots(4, len(parts), figsize=(6 * len(parts), 15))\n",
    "        if len(parts) == 1:\n",
    "            axs = np.array([[axs[0]], [axs[1]], [axs[2]], [axs[3]]])\n",
    "\n",
    "        for col, (param, name) in enumerate(parts.items()):\n",
    "            grads = torch.cat(storage[name])\n",
    "            nonzero = grads[grads != 0]\n",
    "            if nonzero.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            abs_grads = nonzero.abs()\n",
    "            log_grads = torch.log(abs_grads)\n",
    "            m, s = grads.mean().item(), grads.std().item()\n",
    "            mlog, slog = log_grads.mean().item(), log_grads.std().item()\n",
    "\n",
    "            f1, b1 = np.histogram(grads.numpy(), bins=bins, range=(m - 3*s, m + 3*s))\n",
    "            f2, b2 = np.histogram(log_grads.numpy(), bins=bins, range=(mlog - 3*slog, mlog + 3*slog))\n",
    "\n",
    "            axs[0][col].bar(b1[:-1], f1, width=np.diff(b1), edgecolor=\"black\")\n",
    "            axs[0][col].set_title(f\"{gname}: {param}\\nMean={m:.2e}, Std={s:.2e}\")\n",
    "            axs[0][col].set_xlabel(\"Grad\")\n",
    "            axs[0][col].set_ylabel(\"Count\")\n",
    "\n",
    "            axs[1][col].bar(b2[:-1], f2, width=np.diff(b2), edgecolor=\"black\")\n",
    "            axs[1][col].set_title(f\"log(|Grad|): Mean={mlog:.2f}, Std={slog:.2f}\")\n",
    "            axs[1][col].set_xlabel(\"log(|Grad|)\")\n",
    "\n",
    "            for d in ['normal', 'laplace', 't']:\n",
    "                tq, eq = compute_qq(nonzero, d)\n",
    "                axs[2][col].plot(tq, eq, label=d)\n",
    "            axs[2][col].plot(tq, tq, 'k--')\n",
    "            axs[2][col].set_title(\"Q-Q Plot (Centered)\")\n",
    "            axs[2][col].legend()\n",
    "\n",
    "            for d in ['lognorm', 'cauchy', 'pareto']:\n",
    "                tq, eq = compute_qq(abs_grads, d)\n",
    "                axs[3][col].plot(tq, eq, label=d)\n",
    "            axs[3][col].plot(tq, tq, 'k--')\n",
    "            axs[3][col].set_title(\"Q-Q Plot (Heavy-Tailed)\")\n",
    "            axs[3][col].legend()\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_DIR, f\"{gname.replace('.', '_')}_grouped.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# --- Execution --- #\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    data_loader = get_data_loader()\n",
    "    model, optimizer, criterion = setup_model()\n",
    "    grad_storage = defaultdict(list)\n",
    "    register_hooks(model, grad_storage)\n",
    "    train(model, data_loader, optimizer, criterion, grad_storage)\n",
    "    plot_gradients(grad_storage)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
